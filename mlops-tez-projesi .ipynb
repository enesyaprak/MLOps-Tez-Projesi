{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfa8fab2-3d28-4266-8a75-99a7eb7684ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "âœ… Sistem HazÄ±r. Ã‡alÄ±ÅŸma AlanÄ±: s3://sagemaker-us-east-1-437151405779/tez-v2\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import os\n",
    "\n",
    "# Oturum aÃ§\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "region = sagemaker_session.boto_region_name\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "# --- YENÄ° BAÅLANGIÃ‡: Versiyon 2 ---\n",
    "# Eski dosyalarla karÄ±ÅŸmasÄ±n diye v2 klasÃ¶rÃ¼ kullanÄ±yoruz\n",
    "project_prefix = \"tez-v2\"\n",
    "\n",
    "print(f\"âœ… Sistem HazÄ±r. Ã‡alÄ±ÅŸma AlanÄ±: s3://{default_bucket}/{project_prefix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a80a1a9-b6f2-45c2-abd8-baa94a9d319f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dosya bulundu, S3'e yÃ¼kleniyor: s3://sagemaker-us-east-1-437151405779/tez-v2/data/adult.csv\n",
      "âœ… Veri YÃ¼klendi.\n"
     ]
    }
   ],
   "source": [
    "local_path = \"adult.csv\"\n",
    "s3_input_uri = f\"s3://{default_bucket}/{project_prefix}/data/adult.csv\"\n",
    "\n",
    "if os.path.exists(local_path):\n",
    "    print(f\"Dosya bulundu, S3'e yÃ¼kleniyor: {s3_input_uri}\")\n",
    "    sagemaker.s3.S3Uploader.upload(local_path, s3_input_uri)\n",
    "    print(\"âœ… Veri YÃ¼klendi.\")\n",
    "else:\n",
    "    print(\"âŒ HATA: adult.csv dosyasÄ±nÄ± sol tarafa yÃ¼klememiÅŸsin!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eebe6af1-1048-4cb1-a9ee-5e651303647e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocessing.py\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Veri isleme basliyor...\")\n",
    "    base_dir = \"/opt/ml/processing\"\n",
    "    \n",
    "    # KlasÃ¶rleri oluÅŸtur\n",
    "    os.makedirs(f\"{base_dir}/train\", exist_ok=True)\n",
    "    os.makedirs(f\"{base_dir}/test\", exist_ok=True)\n",
    "    \n",
    "    # Veriyi oku (BoÅŸluklarÄ± temizleyerek)\n",
    "    df = pd.read_csv(f\"{base_dir}/input/adult.csv\", header=None, skipinitialspace=True)\n",
    "    \n",
    "    # SÃ¼tun isimleri\n",
    "    columns = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\", \"marital-status\",\n",
    "               \"occupation\", \"relationship\", \"race\", \"sex\", \"capital-gain\", \"capital-loss\",\n",
    "               \"hours-per-week\", \"native-country\", \"income\"]\n",
    "    df.columns = columns\n",
    "\n",
    "    # Hedef deÄŸiÅŸkeni temizle\n",
    "    df['income'] = df['income'].astype(str).str.strip().apply(lambda x: 1 if '>50K' in x else 0)\n",
    "    \n",
    "    # Sadece sayÄ±sal sÃ¼tunlarÄ± al\n",
    "    df = df.replace('?', np.nan).dropna()\n",
    "    df = df.select_dtypes(include=['int64', 'float64'])\n",
    "    \n",
    "    # Train/Test ayÄ±r\n",
    "    train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Kaydet\n",
    "    train.to_csv(f\"{base_dir}/train/train.csv\", header=False, index=False)\n",
    "    test.to_csv(f\"{base_dir}/test/test.csv\", header=False, index=False)\n",
    "    print(f\"âœ… TamamlandÄ±. Train: {len(train)}, Test: {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a40e3b1-dc7c-4204-8237-eb00c0dd91b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Egitim basliyor...\")\n",
    "    \n",
    "    # Okuma\n",
    "    train_data = pd.read_csv(\"/opt/ml/input/data/train/train.csv\", header=None)\n",
    "    val_data = pd.read_csv(\"/opt/ml/input/data/validation/test.csv\", header=None)\n",
    "    \n",
    "    X_train = train_data.iloc[:, :-1]\n",
    "    y_train = train_data.iloc[:, -1]\n",
    "    X_val = val_data.iloc[:, :-1]\n",
    "    y_val = val_data.iloc[:, -1]\n",
    "    \n",
    "    # EÄŸitim\n",
    "    model = xgb.XGBClassifier(objective='binary:logistic', eval_metric='error')\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # KayÄ±t (XGBoost native format - Hata vermez)\n",
    "    model.get_booster().save_model(\"/opt/ml/model/xgboost-model\")\n",
    "    print(\"âœ… Model kaydedildi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13ab96a3-94ac-4b57-b5f6-d021c8f110eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Pipeline v2 FÄ±rlatÄ±ldÄ±! ARN: arn:aws:sagemaker:us-east-1:437151405779:pipeline/Tez-Pipeline-v2/execution/tdnyvj36i3aq\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "# --- SABÄ°T ADRES STRATEJÄ°SÄ° ---\n",
    "# Ä°ÅŸlenen veriler ve modeller tam olarak buraya gidecek.\n",
    "processed_data_uri = f\"s3://{default_bucket}/{project_prefix}/processed\"\n",
    "model_output_uri = f\"s3://{default_bucket}/{project_prefix}/models\"\n",
    "\n",
    "# 1. Ä°ÅŸlemci AyarlarÄ±\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=\"0.23-1\",\n",
    "    role=role,\n",
    "    instance_type=\"ml.t3.medium\",\n",
    "    instance_count=1,\n",
    "    base_job_name=\"clean-process\"\n",
    ")\n",
    "\n",
    "# 2. Veri Ä°ÅŸleme AdÄ±mÄ± (Ã‡Ä±ktÄ± adresini elle veriyoruz)\n",
    "step_process = ProcessingStep(\n",
    "    name=\"VeriIsleme_v2\",\n",
    "    processor=sklearn_processor,\n",
    "    inputs=[ProcessingInput(source=s3_input_uri, destination=\"/opt/ml/processing/input\")],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\", destination=f\"{processed_data_uri}/train\"),\n",
    "        ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\", destination=f\"{processed_data_uri}/test\")\n",
    "    ],\n",
    "    code=\"preprocessing.py\"\n",
    ")\n",
    "\n",
    "# 3. EÄŸitim AyarlarÄ±\n",
    "xgb_estimator = Estimator(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    image_uri=sagemaker.image_uris.retrieve(\"xgboost\", region, \"1.5-1\"),\n",
    "    output_path=model_output_uri,\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\".\"\n",
    ")\n",
    "\n",
    "# 4. EÄŸitim AdÄ±mÄ± (Girdi adresini elle veriyoruz)\n",
    "step_train = TrainingStep(\n",
    "    name=\"ModelEgitimi_v2\",\n",
    "    estimator=xgb_estimator,\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(s3_data=f\"{processed_data_uri}/train\", content_type=\"text/csv\"),\n",
    "        \"validation\": TrainingInput(s3_data=f\"{processed_data_uri}/test\", content_type=\"text/csv\")\n",
    "    },\n",
    "    depends_on=[step_process] # Ä°ÅŸleme bitmeden baÅŸlama\n",
    ")\n",
    "\n",
    "# 5. BaÅŸlat\n",
    "pipeline = Pipeline(\n",
    "    name=\"Tez-Pipeline-v2\",\n",
    "    steps=[step_process, step_train],\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "pipeline.upsert(role_arn=role)\n",
    "execution = pipeline.start()\n",
    "print(f\"ğŸš€ Pipeline v2 FÄ±rlatÄ±ldÄ±! ARN: {execution.arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b37b052-1ce1-4056-81fb-479d40be48b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model Adresi Bulundu: s3://sagemaker-us-east-1-437151405779/tez-v2/models/pipelines-tdnyvj36i3aq-ModelEgitimi-v2-ufDbKvfatn/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# Son Ã§alÄ±ÅŸan adÄ±mlarÄ± listele\n",
    "steps = execution.list_steps()\n",
    "\n",
    "# EÄŸitim adÄ±mÄ±nÄ± bul\n",
    "training_step = next(s for s in steps if s['StepName'] == 'ModelEgitimi_v2')\n",
    "\n",
    "# EÄŸitim iÅŸinin detaylarÄ±na inip modelin S3 adresini alÄ±yoruz\n",
    "training_job_arn = training_step['Metadata']['TrainingJob']['Arn']\n",
    "training_job_name = training_job_arn.split('/')[-1]\n",
    "desc = sagemaker_session.sagemaker_client.describe_training_job(TrainingJobName=training_job_name)\n",
    "model_s3_uri = desc['ModelArtifacts']['S3ModelArtifacts']\n",
    "\n",
    "print(f\"âœ… Model Adresi Bulundu: {model_s3_uri}\")\n",
    "\n",
    "# Modelin Ã§alÄ±ÅŸacaÄŸÄ± imajÄ± (XGBoost) tekrar Ã§aÄŸÄ±rÄ±yoruz\n",
    "image_uri = sagemaker.image_uris.retrieve(\"xgboost\", region, \"1.5-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cdbd722-5e03-49ad-baf8-32de12683167",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-xgboost-2025-12-01-22-02-15-314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Endpoint kuruluyor... (Bu iÅŸlem 5-10 dk sÃ¼rebilir, kahveni tazele â˜•)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating endpoint-config with name sagemaker-xgboost-2025-12-01-22-02-16-058\n",
      "INFO:sagemaker:Creating endpoint with name sagemaker-xgboost-2025-12-01-22-02-16-058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff0000; text-decoration-color: #ff0000\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ </span><span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">Traceback </span><span style=\"color: #ff7f7f; text-decoration-color: #ff7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\"> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span> in &lt;module&gt;:19                                                                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">16 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   </span>instance_type=<span style=\"color: #808000; text-decoration-color: #808000\">\"ml.t2.medium\"</span>                                                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">17 </span>)                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">18 </span>                                                                                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span>19 <span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(<span style=\"color: #808000; text-decoration-color: #808000\">f\"\\nâœ… Endpoint BaÅŸarÄ±yla Kuruldu! AdÄ±: {</span>predi<span style=\"font-weight: bold; text-decoration: underline\">ctor.endpoint_name</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold; text-decoration: underline\">}\"</span><span style=\"font-weight: bold; text-decoration: underline\">)</span>                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">20 </span>                                                                                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">AttributeError: </span><span style=\"color: #008700; text-decoration-color: #008700\">'NoneType'</span> object has no attribute <span style=\"color: #008700; text-decoration-color: #008700\">'endpoint_name'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;0;0mâ•­â”€\u001b[0m\u001b[38;2;255;0;0mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[38;2;255;0;0m \u001b[0m\u001b[1;38;2;255;0;0mTraceback \u001b[0m\u001b[1;2;38;2;255;0;0m(most recent call last)\u001b[0m\u001b[38;2;255;0;0m \u001b[0m\u001b[38;2;255;0;0mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[38;2;255;0;0mâ”€â•®\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m in <module>:19                                                                                   \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m                                                                                                  \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m16 \u001b[0m\u001b[2mâ”‚   \u001b[0minstance_type=\u001b[33m\"\u001b[0m\u001b[33mml.t2.medium\u001b[0m\u001b[33m\"\u001b[0m                                                            \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m17 \u001b[0m)                                                                                           \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m18 \u001b[0m                                                                                            \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m \u001b[31mâ± \u001b[0m19 \u001b[96mprint\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m\\n\u001b[0m\u001b[33mâœ… Endpoint BaÅŸarÄ±yla Kuruldu! AdÄ±: \u001b[0m\u001b[33m{\u001b[0mpredi\u001b[1;4mctor.endpoint_name\u001b[0m\u001b[1;4;33m}\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4m)\u001b[0m                   \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m20 \u001b[0m                                                                                            \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n",
       "\u001b[1;91mAttributeError: \u001b[0m\u001b[38;2;0;135;0m'NoneType'\u001b[0m object has no attribute \u001b[38;2;0;135;0m'endpoint_name'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sagemaker.model import Model\n",
    "\n",
    "# Model nesnesini hazÄ±rlÄ±yoruz\n",
    "model = Model(\n",
    "    image_uri=image_uri,\n",
    "    model_data=model_s3_uri,\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "print(f\"ğŸš€ Endpoint kuruluyor... (Bu iÅŸlem 5-10 dk sÃ¼rebilir, kahveni tazele â˜•)\")\n",
    "\n",
    "# Deploy ediyoruz (Free tier dostu makineyle)\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.t2.medium\"\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Endpoint BaÅŸarÄ±yla Kuruldu! AdÄ±: {predictor.endpoint_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20af9939-c2e8-4b54-9969-e14e9b589ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… BULUNDU! Aktif Endpoint: sagemaker-xgboost-2025-12-01-22-02-16-058\n",
      "ğŸš€ BaÄŸlantÄ± kuruldu! Åimdi HÃ¼cre 9 ile test edebilirsin.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "\n",
    "# 1. AWS'ye sor: \"Ã‡alÄ±ÅŸan en yeni endpoint hangisi?\"\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "endpoints = sm_client.list_endpoints(SortBy='CreationTime', SortOrder='Descending', StatusEquals='InService')\n",
    "\n",
    "if len(endpoints['Endpoints']) > 0:\n",
    "    # Ä°smi otomatik bulduk\n",
    "    endpoint_ismi = endpoints['Endpoints'][0]['EndpointName']\n",
    "    print(f\"âœ… BULUNDU! Aktif Endpoint: {endpoint_ismi}\")\n",
    "    \n",
    "    # 2. BaÄŸlanÄ±yoruz (KumandayÄ± geri alÄ±yoruz)\n",
    "    predictor = Predictor(\n",
    "        endpoint_name=endpoint_ismi,\n",
    "        sagemaker_session=sagemaker_session\n",
    "    )\n",
    "    predictor.serializer = CSVSerializer()\n",
    "    \n",
    "    print(\"ğŸš€ BaÄŸlantÄ± kuruldu! Åimdi HÃ¼cre 9 ile test edebilirsin.\")\n",
    "else:\n",
    "    print(\"âŒ HATA: Åu an 'InService' durumunda endpoint bulunamadÄ±. Biraz daha beklemen gerekebilir.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bce75643-2fd5-4df7-91f9-1ae1e416da1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â“ Soru Soruluyor: 39, 77516, 13, 2174, 0, 40\n",
      "ğŸ¤– Modelin Skoru: 0.004923452157527208\n",
      "ğŸ“‰ TAHMÄ°N: <=50K (Fakir)\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.serializers import CSVSerializer\n",
    "\n",
    "# Sunucuyla CSV diliyle konuÅŸacaÄŸÄ±mÄ±zÄ± belirtiyoruz\n",
    "predictor.serializer = CSVSerializer()\n",
    "\n",
    "# TEST VERÄ°SÄ°:\n",
    "# 39 YaÅŸÄ±nda, 77K gelirli, 13 yÄ±l okumuÅŸ, sermaye kazancÄ± 2174 dolar vb.\n",
    "test_verisi = \"39, 77516, 13, 2174, 0, 40\" \n",
    "\n",
    "print(f\"â“ Soru Soruluyor: {test_verisi}\")\n",
    "tahmin = predictor.predict(test_verisi)\n",
    "sonuc = float(tahmin.decode('utf-8'))\n",
    "\n",
    "print(f\"ğŸ¤– Modelin Skoru: {sonuc}\")\n",
    "\n",
    "if sonuc > 0.5:\n",
    "    print(\"ğŸ’° TAHMÄ°N: >50K (Zengin)\")\n",
    "else:\n",
    "    print(\"ğŸ“‰ TAHMÄ°N: <=50K (Fakir)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51709925-20dd-42b4-a438-c6b8b4091e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Rol Geri Geldi: arn:aws:iam::437151405779:role/service-role/AmazonSageMaker-ExecutionRole-20251201T152140\n",
      "âœ… Bucket: sagemaker-us-east-1-437151405779\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# HafÄ±zayÄ± geri getiriyoruz\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "region = sagemaker_session.boto_region_name\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "print(f\"âœ… Rol Geri Geldi: {role}\")\n",
    "print(f\"âœ… Bucket: {default_bucket}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0e03a63-3c97-4ee6-98c3-6f5e666b75bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ 'adult.csv' bulundu. S3'e yÃ¼kleniyor...\n",
      "âœ… Dosya baÅŸarÄ±yla ÅŸuraya yÃ¼klendi: s3://sagemaker-us-east-1-437151405779/tez-v2/data/adult.csv\n",
      "ğŸ“ Baseline (Referans) oluÅŸturma iÅŸlemi baÅŸlÄ±yor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name baseline-suggestion-job-2025-12-01-22-11-08-479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".......................\u001b[34m2025-12-01 22:14:54.293355: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2025-12-01 22:14:54.293404: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\u001b[0m\n",
      "\u001b[34m2025-12-01 22:14:56.126298: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2025-12-01 22:14:56.126344: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:14:56.126374: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-2-72-120.ec2.internal): /proc/driver/nvidia/version does not exist\u001b[0m\n",
      "\u001b[34m2025-12-01 22:14:56.126715: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\u001b[0m\n",
      "\u001b[34mTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[0m\n",
      "\u001b[34m2025-12-01 22:14:58,045 - __main__ - INFO - All params:{'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:437151405779:processing-job/baseline-suggestion-job-2025-12-01-22-11-08-479', 'ProcessingJobName': 'baseline-suggestion-job-2025-12-01-22-11-08-479', 'Environment': {'dataset_format': '{\"csv\": {\"header\": false, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}, 'AppSpecification': {'ImageUri': '156813124566.dkr.ecr.us-east-1.amazonaws.com/sagemaker-model-monitor-analyzer', 'ContainerEntrypoint': None, 'ContainerArguments': None}, 'ProcessingInputs': [{'InputName': 'baseline_dataset_input', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/input/baseline_dataset_input', 'S3Uri': 's3://sagemaker-us-east-1-437151405779/tez-v2/data/adult.csv', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}, 'DatasetDefinitionInput': None}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'monitoring_output', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/output', 'S3Uri': 's3://sagemaker-us-east-1-437151405779/tez-v2/model-monitor/baseline', 'S3UploadMode': 'EndOfJob'}, 'FeatureStoreOutput': None}], 'KmsKeyId': None}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1, 'InstanceType': 'ml.t3.large', 'VolumeSizeInGB': 20, 'VolumeKmsKeyId': None}}, 'NetworkConfig': {'VpcConfig': None, 'EnableNetworkIsolation': False, 'EnableInterContainerTrafficEncryption': False}, 'RoleArn': 'arn:aws:iam::437151405779:role/service-role/AmazonSageMaker-ExecutionRole-20251201T152140', 'StoppingCondition': {'MaxRuntimeInSeconds': 3600}}\u001b[0m\n",
      "\u001b[34m2025-12-01 22:14:58,046 - __main__ - INFO - Current Environment:{'dataset_format': '{\"csv\": {\"header\": false, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}\u001b[0m\n",
      "\u001b[34m2025-12-01 22:14:58,046 - __main__ - INFO - categorical_drift_method:None\u001b[0m\n",
      "\u001b[34m2025-12-01 22:14:58,046 - DefaultDataAnalyzer - INFO - Performing analysis with input: {\"dataset_source\": \"/opt/ml/processing/input/baseline_dataset_input\", \"dataset_format\": {\"csv\": {\"header\": false, \"output_columns_position\": \"START\"}}, \"output_path\": \"/opt/ml/processing/output\", \"monitoring_input_type\": null, \"analysis_type\": null, \"problem_type\": null, \"inference_attribute\": null, \"probability_attribute\": null, \"ground_truth_attribute\": null, \"probability_threshold_attribute\": null, \"positive_label\": null, \"exclude_features_attribute\": null, \"record_preprocessor_script\": null, \"post_analytics_processor_script\": null, \"baseline_constraints\": null, \"baseline_statistics\": null, \"data_quality_monitoring_config\": {\"evaluate_constraints\": \"Enabled\", \"emit_metrics\": \"Enabled\", \"datatype_check_threshold\": 1.0, \"domain_content_threshold\": 1.0, \"distribution_constraints\": {\"perform_comparison\": \"Enabled\", \"comparison_threshold\": 0.1, \"comparison_method\": \"Robust\", \"categorical_comparison_threshold\": 0.1, \"categorical_drift_method\": \"LInfinity\"}}, \"start_time\": null, \"end_time\": null, \"metric_time\": null, \"cloudwatch_metrics_directory\": \"/opt/ml/output/metrics/cloudwatch\", \"publish_cloudwatch_metrics\": \"Disabled\", \"sagemaker_endpoint_name\": null, \"sagemaker_monitoring_schedule_name\": null, \"output_message_file\": \"/opt/ml/output/message\", \"detect_outliers\": null, \"detect_drift\": null, \"image_data\": null, \"report_enabled\": false, \"auto_ml_job_detail\": null}\u001b[0m\n",
      "\u001b[34m2025-12-01 22:14:58,046 - DefaultDataAnalyzer - INFO - Bootstrapping yarn\u001b[0m\n",
      "\u001b[34m2025-12-01 22:14:58,046 - bootstrap - INFO - Copy aws jars\u001b[0m\n",
      "\u001b[34m2025-12-01 22:14:58,291 - bootstrap - INFO - Copy cluster config\u001b[0m\n",
      "\u001b[34m2025-12-01 22:14:58,294 - bootstrap - INFO - Write runtime cluster config\u001b[0m\n",
      "\u001b[34m2025-12-01 22:14:58,295 - bootstrap - INFO - Resource Config is: {'current_host': 'algo-1', 'current_instance_type': 'ml.t3.large', 'current_group_name': 'homogeneousCluster', 'hosts': ['algo-1'], 'instance_groups': [{'instance_group_name': 'homogeneousCluster', 'instance_type': 'ml.t3.large', 'hosts': ['algo-1']}], 'network_interface_name': 'eth0', 'topology': None}\u001b[0m\n",
      "\u001b[34m2025-12-01 22:14:58,308 - bootstrap - INFO - Finished Yarn configuration files setup.\u001b[0m\n",
      "\u001b[34m2025-12-01 22:14:58,308 - bootstrap - INFO - Starting spark process for master node algo-1\u001b[0m\n",
      "\u001b[34m2025-12-01 22:14:58,309 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs namenode -format -force\u001b[0m\n",
      "\u001b[34mWARNING: /usr/hadoop-3.0.0/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:00,506 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.2.72.120\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = [-format, -force]\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.0.0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/hadoop-3.0.0/etc/hadoop:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/junit-4.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-aws-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/aws-java-sdk-bundle-1.11.199.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-kms-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okio-1.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-annotations-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/findbugs-annotations-1.3.9-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-csv-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-prefix-tree-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-2.2.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-protocol-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jcodings-1.0.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-common-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop2-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-compiler-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-el-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/disruptor-3.3.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/htrace-core-3.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/fst-2.50.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/joni-2.1.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-math-2.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-runtime-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-server-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/servlet-api-2.5-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-procedure-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-client-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-api-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jamon-runtime-2.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-registry-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn\u001b[0m\n",
      "\u001b[34m/hadoop-yarn-server-timelineservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-router-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-api-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-3.0.0.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r c25427ceca461ee979d30edd7a4b0f50718e6533; compiled by 'andrew' on 2017-12-08T19:16Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_462\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:00,531 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:00,540 INFO namenode.NameNode: createNameNode [-format, -force]\u001b[0m\n",
      "\u001b[34mFormatting using clusterid: CID-d96a7e2f-e04e-4f18-9f9a-b92a3173690f\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:02,196 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:02,234 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:02,240 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:02,250 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:02,271 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:02,271 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:02,271 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:02,271 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:02,389 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:02,449 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:02,449 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:02,467 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:02,475 INFO blockmanagement.BlockManager: The block deletion will start around 2025 Dec 01 22:15:02\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:02,477 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:02,478 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:02,486 INFO util.GSet: 2.0% max memory 1.4 GB = 28.5 MB\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:02,487 INFO util.GSet: capacity      = 2^22 = 4194304 entries\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:02,549 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:02,562 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:02,563 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:02,563 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:02,563 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:02,563 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:02,563 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:02,563 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:02,563 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:02,563 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:02,563 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:02,563 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:02,675 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:02,676 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:02,676 INFO util.GSet: 1.0% max memory 1.4 GB = 14.2 MB\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:02,676 INFO util.GSet: capacity      = 2^21 = 2097152 entries\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:02,678 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:02,678 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:02,678 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:02,678 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:02,689 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:02,696 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:02,696 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:02,697 INFO util.GSet: 0.25% max memory 1.4 GB = 3.6 MB\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:02,697 INFO util.GSet: capacity      = 2^19 = 524288 entries\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:02,712 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:02,712 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:02,712 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:02,717 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:02,717 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:02,721 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:02,721 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:02,722 INFO util.GSet: 0.029999999329447746% max memory 1.4 GB = 437.5 KB\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:02,722 INFO util.GSet: capacity      = 2^16 = 65536 entries\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:02,774 INFO namenode.FSImage: Allocated new BlockPoolId: BP-424728387-10.2.72.120-1764627302753\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:02,805 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:02,825 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:03,012 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 389 bytes saved in 0 seconds.\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:03,045 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:03,060 INFO namenode.NameNode: SHUTDOWN_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSHUTDOWN_MSG: Shutting down NameNode at algo-1/10.2.72.120\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:03,075 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:05,188 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode, return code 1\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:05,188 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:07,406 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode, return code 1\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:07,406 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:09,790 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager, return code 1\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:09,791 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:12,346 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager, return code 1\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:12,346 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:14,996 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver, return code 1\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:14,997 - DefaultDataAnalyzer - INFO - Total number of hosts in the cluster: 1\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:25,005 - DefaultDataAnalyzer - INFO - Running command: bin/spark-submit --master yarn --deploy-mode client --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider --conf spark.serializer=org.apache.spark.serializer.KryoSerializer /opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:28,517 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:29,531 INFO Main: Start analyzing with args: --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:29,588 INFO Main: Analytics input path: DataAnalyzerParams(/tmp/spark_job_config.json,yarn)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:29,614 INFO FileUtil: Read file from path /tmp/spark_job_config.json.\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:30,508 INFO spark.SparkContext: Running Spark version 3.3.0\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:30,546 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:30,547 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:30,547 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:30,548 INFO spark.SparkContext: Submitted application: SageMakerDataAnalyzer\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:30,591 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 5744, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:30,608 INFO resource.ResourceProfile: Limiting resource is cpus at 1 tasks per executor\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:30,612 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:30,756 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:30,756 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:30,757 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:30,757 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:30,758 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:31,423 INFO util.Utils: Successfully started service 'sparkDriver' on port 34851.\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:31,501 INFO spark.SparkEnv: Registering MapOutputTracker\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:31,583 INFO spark.SparkEnv: Registering BlockManagerMaster\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:31,620 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:31,621 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:31,679 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:31,737 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-e20f9206-362f-4d5d-9a73-112cae7348ab\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:31,770 INFO memory.MemoryStore: MemoryStore started with capacity 1458.6 MiB\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:31,840 INFO spark.SparkEnv: Registering OutputCommitCoordinator\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:31,907 INFO spark.SparkContext: Added JAR file:/opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar at spark://10.2.72.120:34851/jars/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar with timestamp 1764627330498\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:32,923 INFO client.RMProxy: Connecting to ResourceManager at /10.2.72.120:8032\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:34,051 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:34,052 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:34,061 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (7834 MB per container)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:34,062 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:34,062 INFO yarn.Client: Setting up container launch context for our AM\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:34,063 INFO yarn.Client: Setting up the launch environment for our AM container\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:34,071 INFO yarn.Client: Preparing resources for our AM container\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:34,188 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:36,770 INFO yarn.Client: Uploading resource file:/tmp/spark-13a073ce-2ac2-4f7c-a73e-931e735513a7/__spark_libs__3187359236634857415.zip -> hdfs://10.2.72.120/user/root/.sparkStaging/application_1764627311661_0001/__spark_libs__3187359236634857415.zip\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:39,189 INFO yarn.Client: Uploading resource file:/tmp/spark-13a073ce-2ac2-4f7c-a73e-931e735513a7/__spark_conf__2060351632886642640.zip -> hdfs://10.2.72.120/user/root/.sparkStaging/application_1764627311661_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:39,647 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:39,648 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:39,648 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:39,648 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:39,649 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:39,700 INFO yarn.Client: Submitting application application_1764627311661_0001 to ResourceManager\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:39,990 INFO impl.YarnClientImpl: Submitted application application_1764627311661_0001\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:40,999 INFO yarn.Client: Application report for application_1764627311661_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:41,011 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: [Mon Dec 01 22:15:40 +0000 2025] Scheduler has assigned a container for AM, waiting for AM container to be launched\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1764627339841\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1764627311661_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:42,018 INFO yarn.Client: Application report for application_1764627311661_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:43,023 INFO yarn.Client: Application report for application_1764627311661_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:44,033 INFO yarn.Client: Application report for application_1764627311661_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:45,038 INFO yarn.Client: Application report for application_1764627311661_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:46,042 INFO yarn.Client: Application report for application_1764627311661_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:47,053 INFO yarn.Client: Application report for application_1764627311661_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:48,060 INFO yarn.Client: Application report for application_1764627311661_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:48,752 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1764627311661_0001), /proxy/application_1764627311661_0001\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:49,064 INFO yarn.Client: Application report for application_1764627311661_0001 (state: RUNNING)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:49,064 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: 10.2.72.120\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1764627339841\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1764627311661_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:49,066 INFO cluster.YarnClientSchedulerBackend: Application application_1764627311661_0001 has started running.\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:49,130 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41083.\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:49,130 INFO netty.NettyBlockTransferService: Server created on 10.2.72.120:41083\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:49,133 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:49,156 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.2.72.120, 41083, None)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:49,171 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.2.72.120:41083 with 1458.6 MiB RAM, BlockManagerId(driver, 10.2.72.120, 41083, None)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:49,182 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.2.72.120, 41083, None)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:49,186 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.2.72.120, 41083, None)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:49,459 INFO util.log: Logging initialized @23992ms to org.sparkproject.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:51,587 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:57,248 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.2.72.120:58308) with ID 1,  ResourceProfileId 0\u001b[0m\n",
      "\u001b[34m2025-12-01 22:15:57,734 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-1:41995 with 2.8 GiB RAM, BlockManagerId(1, algo-1, 41995, None)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:02,737 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:03,226 WARN spark.SparkContext: Spark is not running in local mode, therefore the checkpoint directory must not be on the local filesystem. Directory '/tmp' appears to be on the local filesystem.\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:03,335 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:03,346 INFO internal.SharedState: Warehouse path is 'file:/usr/spark-3.3.0/spark-warehouse'.\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:05,500 INFO datasources.InMemoryFileIndex: It took 119 ms to list leaf files for 1 paths.\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:05,905 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 416.9 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:06,549 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 39.2 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:06,554 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.2.72.120:41083 (size: 39.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:06,565 INFO spark.SparkContext: Created broadcast 0 from csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:07,236 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:07,239 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:07,255 INFO input.CombineFileInputFormat: DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:07,374 INFO spark.SparkContext: Starting job: csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:07,399 INFO scheduler.DAGScheduler: Got job 0 (csv at DatasetReader.scala:99) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:07,401 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (csv at DatasetReader.scala:99)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:07,403 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:07,405 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:07,413 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:07,553 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.3 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:07,565 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:07,567 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.2.72.120:41083 (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:07,568 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:07,604 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:07,607 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:07,679 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4618 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:08,124 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-1:41995 (size: 4.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:09,463 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-1:41995 (size: 39.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:10,130 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2476 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:10,133 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:10,152 INFO scheduler.DAGScheduler: ResultStage 0 (csv at DatasetReader.scala:99) finished in 2.659 s\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:10,164 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:10,165 INFO cluster.YarnScheduler: Killing all running tasks in stage 0: Stage finished\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:10,168 INFO scheduler.DAGScheduler: Job 0 finished: csv at DatasetReader.scala:99, took 2.793192 s\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:10,542 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on algo-1:41995 in memory (size: 4.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:10,561 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 10.2.72.120:41083 in memory (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:15,126 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:15,129 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:15,136 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: string, _c1: string, _c2: string, _c3: string, _c4: string ... 13 more fields>\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:15,537 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 416.5 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:15,562 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 39.1 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:15,566 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.2.72.120:41083 (size: 39.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:15,569 INFO spark.SparkContext: Created broadcast 2 from head at DataAnalyzer.scala:124\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:15,589 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4735834 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:15,659 INFO spark.SparkContext: Starting job: head at DataAnalyzer.scala:124\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:15,661 INFO scheduler.DAGScheduler: Got job 1 (head at DataAnalyzer.scala:124) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:15,661 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (head at DataAnalyzer.scala:124)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:15,662 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:15,665 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:15,672 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:124), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:15,812 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 17.6 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:15,832 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.2 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:15,835 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.2.72.120:41083 (size: 8.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:15,837 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:15,840 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:124) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:15,840 INFO cluster.YarnScheduler: Adding task set 1.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:15,851 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:15,947 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-1:41995 (size: 8.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:17,676 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-1:41995 (size: 39.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:18,867 INFO storage.BlockManagerInfo: Added rdd_7_0 in memory on algo-1:41995 (size: 1767.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:19,041 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 3197 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:19,043 INFO scheduler.DAGScheduler: ResultStage 1 (head at DataAnalyzer.scala:124) finished in 3.363 s\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:19,045 INFO scheduler.DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:19,046 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:19,046 INFO cluster.YarnScheduler: Killing all running tasks in stage 1: Stage finished\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:19,047 INFO scheduler.DAGScheduler: Job 1 finished: head at DataAnalyzer.scala:124, took 3.387701 s\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:19,475 INFO codegen.CodeGenerator: Code generated in 320.53413 ms\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:19,865 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on algo-1:41995 in memory (size: 8.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:19,873 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.2.72.120:41083 in memory (size: 8.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:20,944 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:21,339 INFO scheduler.DAGScheduler: Registering RDD 16 (collect at AnalysisRunner.scala:326) as input to shuffle 0\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:21,344 INFO scheduler.DAGScheduler: Got map stage job 2 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:21,345 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:21,346 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:21,350 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:21,354 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:21,395 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 114.5 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:21,400 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:21,401 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.2.72.120:41083 (size: 34.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:21,404 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:21,408 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:21,408 INFO cluster.YarnScheduler: Adding task set 2.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:21,419 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:21,455 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-1:41995 (size: 34.7 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:24,678 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 3261 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:24,687 INFO scheduler.DAGScheduler: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326) finished in 3.325 s\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:24,688 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:24,689 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:24,689 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:24,689 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:24,693 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:24,930 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:24,934 INFO scheduler.DAGScheduler: Got job 3 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:24,934 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:24,937 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:24,938 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:24,943 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:24,975 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 167.0 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:24,978 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 45.8 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:24,980 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.2.72.120:41083 (size: 45.8 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:24,981 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:24,982 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:24,982 INFO cluster.YarnScheduler: Adding task set 4.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:24,987 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:25,034 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-1:41995 (size: 45.8 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:25,095 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.2.72.120:58308\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:25,683 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 697 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:25,685 INFO scheduler.DAGScheduler: ResultStage 4 (collect at AnalysisRunner.scala:326) finished in 0.724 s\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:25,686 INFO scheduler.DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:25,686 INFO cluster.YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:25,688 INFO cluster.YarnScheduler: Killing all running tasks in stage 4: Stage finished\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:25,688 INFO scheduler.DAGScheduler: Job 3 finished: collect at AnalysisRunner.scala:326, took 0.757858 s\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:25,778 INFO codegen.CodeGenerator: Code generated in 79.232491 ms\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:26,138 INFO codegen.CodeGenerator: Code generated in 28.216002 ms\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:26,252 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:26,255 INFO scheduler.DAGScheduler: Got job 4 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:26,257 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:26,257 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:26,258 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:26,260 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:26,297 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 37.3 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:26,303 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 16.4 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:26,308 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.2.72.120:41083 (size: 16.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:26,308 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:26,315 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:26,315 INFO cluster.YarnScheduler: Adding task set 5.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:26,318 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:26,346 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-1:41995 (size: 16.4 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:30,165 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 3848 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:30,166 INFO cluster.YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:30,166 INFO scheduler.DAGScheduler: ResultStage 5 (treeReduce at KLLRunner.scala:107) finished in 3.905 s\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:30,167 INFO scheduler.DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:30,167 INFO cluster.YarnScheduler: Killing all running tasks in stage 5: Stage finished\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:30,167 INFO scheduler.DAGScheduler: Job 4 finished: treeReduce at KLLRunner.scala:107, took 3.914929 s\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:30,808 INFO codegen.CodeGenerator: Code generated in 128.315357 ms\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:30,834 INFO scheduler.DAGScheduler: Registering RDD 34 (collect at AnalysisRunner.scala:326) as input to shuffle 1\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:30,841 INFO scheduler.DAGScheduler: Got map stage job 5 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:30,841 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:30,842 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:30,845 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:30,848 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:30,873 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 54.5 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:30,876 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 19.2 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:30,883 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.2.72.120:41083 (size: 19.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:30,887 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:30,896 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:30,896 INFO cluster.YarnScheduler: Adding task set 6.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:30,909 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:30,944 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-1:41995 (size: 19.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:31,378 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 470 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:31,379 INFO cluster.YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:31,380 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326) finished in 0.526 s\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:31,380 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:31,380 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:31,380 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:31,380 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:31,770 INFO codegen.CodeGenerator: Code generated in 201.755204 ms\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:31,820 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:31,822 INFO scheduler.DAGScheduler: Got job 6 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:31,822 INFO scheduler.DAGScheduler: Final stage: ResultStage 8 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:31,822 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:31,822 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:31,823 INFO scheduler.DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:31,841 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 44.7 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:31,848 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 14.3 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:31,849 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.2.72.120:41083 (size: 14.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:31,851 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:31,851 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:31,851 INFO cluster.YarnScheduler: Adding task set 8.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:31,858 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 6) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:31,886 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-1:41995 (size: 14.3 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:31,911 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.2.72.120:58308\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:32,115 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 6) in 257 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:32,115 INFO cluster.YarnScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:32,116 INFO scheduler.DAGScheduler: ResultStage 8 (collect at AnalysisRunner.scala:326) finished in 0.292 s\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:32,118 INFO scheduler.DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:32,118 INFO cluster.YarnScheduler: Killing all running tasks in stage 8: Stage finished\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:32,119 INFO scheduler.DAGScheduler: Job 6 finished: collect at AnalysisRunner.scala:326, took 0.297970 s\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:32,230 INFO codegen.CodeGenerator: Code generated in 82.096877 ms\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:32,556 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:32,572 INFO scheduler.DAGScheduler: Registering RDD 45 (countByKey at ColumnProfiler.scala:592) as input to shuffle 2\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:32,573 INFO scheduler.DAGScheduler: Got job 7 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:32,573 INFO scheduler.DAGScheduler: Final stage: ResultStage 10 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:32,573 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:32,574 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:32,577 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[45] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:32,587 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 30.7 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:32,593 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 14.0 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:32,594 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.2.72.120:41083 (size: 14.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:32,601 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:32,602 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[45] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:32,603 INFO cluster.YarnScheduler: Adding task set 9.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:32,605 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 7) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:32,626 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-1:41995 (size: 14.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:33,340 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 7) in 735 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:33,340 INFO cluster.YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:33,342 INFO scheduler.DAGScheduler: ShuffleMapStage 9 (countByKey at ColumnProfiler.scala:592) finished in 0.763 s\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:33,344 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:33,345 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:33,345 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 10)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:33,346 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:33,346 INFO scheduler.DAGScheduler: Submitting ResultStage 10 (ShuffledRDD[46] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:33,350 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 5.1 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:33,352 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:33,353 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.2.72.120:41083 (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:33,354 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:33,355 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (ShuffledRDD[46] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:33,355 INFO cluster.YarnScheduler: Adding task set 10.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:33,357 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 10.0 (TID 8) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:33,381 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-1:41995 (size: 3.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:33,395 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 10.2.72.120:58308\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:33,480 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 10.0 (TID 8) in 124 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:33,483 INFO cluster.YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:33,484 INFO scheduler.DAGScheduler: ResultStage 10 (countByKey at ColumnProfiler.scala:592) finished in 0.136 s\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:33,485 INFO scheduler.DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:33,485 INFO cluster.YarnScheduler: Killing all running tasks in stage 10: Stage finished\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:33,486 INFO scheduler.DAGScheduler: Job 7 finished: countByKey at ColumnProfiler.scala:592, took 0.926566 s\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:33,822 INFO scheduler.DAGScheduler: Registering RDD 51 (collect at AnalysisRunner.scala:326) as input to shuffle 3\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:33,822 INFO scheduler.DAGScheduler: Got map stage job 8 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:33,823 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:33,823 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:33,824 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:33,826 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:33,837 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 83.5 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:33,841 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 27.2 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:33,843 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.2.72.120:41083 (size: 27.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:33,845 INFO spark.SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:33,847 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:33,847 INFO cluster.YarnScheduler: Adding task set 11.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:33,850 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 11.0 (TID 9) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:33,869 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-1:41995 (size: 27.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:34,411 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 11.0 (TID 9) in 561 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:34,412 INFO scheduler.DAGScheduler: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326) finished in 0.581 s\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:34,413 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:34,414 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:34,414 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:34,414 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:34,414 INFO cluster.YarnScheduler: Removed TaskSet 11.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:34,472 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:34,474 INFO scheduler.DAGScheduler: Got job 9 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:34,475 INFO scheduler.DAGScheduler: Final stage: ResultStage 13 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:34,475 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:34,475 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:34,476 INFO scheduler.DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:34,487 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 168.1 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:34,490 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 46.1 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:34,491 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.2.72.120:41083 (size: 46.1 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:34,493 INFO spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:34,493 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:34,494 INFO cluster.YarnScheduler: Adding task set 13.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:34,497 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 13.0 (TID 10) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:34,517 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on algo-1:41995 (size: 46.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:34,552 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 10.2.72.120:58308\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:34,729 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 13.0 (TID 10) in 233 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:34,729 INFO cluster.YarnScheduler: Removed TaskSet 13.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:34,731 INFO scheduler.DAGScheduler: ResultStage 13 (collect at AnalysisRunner.scala:326) finished in 0.252 s\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:34,731 INFO scheduler.DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:34,735 INFO cluster.YarnScheduler: Killing all running tasks in stage 13: Stage finished\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:34,735 INFO scheduler.DAGScheduler: Job 9 finished: collect at AnalysisRunner.scala:326, took 0.263341 s\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:34,914 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:34,916 INFO scheduler.DAGScheduler: Registering RDD 62 (countByKey at ColumnProfiler.scala:592) as input to shuffle 4\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:34,917 INFO scheduler.DAGScheduler: Got job 10 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:34,917 INFO scheduler.DAGScheduler: Final stage: ResultStage 15 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:34,918 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 14)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:34,918 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 14)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:34,920 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 14 (MapPartitionsRDD[62] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:34,924 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 10.2.72.120:41083 in memory (size: 14.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:34,928 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on algo-1:41995 in memory (size: 14.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:34,938 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 30.7 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:34,941 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 14.0 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:34,942 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.2.72.120:41083 (size: 14.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:34,944 INFO spark.SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:34,945 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[62] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:34,948 INFO cluster.YarnScheduler: Adding task set 14.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:34,950 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 14.0 (TID 11) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:34,968 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on algo-1:41995 (size: 14.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:35,023 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on algo-1:41995 in memory (size: 34.7 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:35,031 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.2.72.120:41083 in memory (size: 34.7 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:35,155 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on 10.2.72.120:41083 in memory (size: 3.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:35,169 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on algo-1:41995 in memory (size: 3.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:35,220 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on 10.2.72.120:41083 in memory (size: 27.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:35,231 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on algo-1:41995 in memory (size: 27.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:35,272 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on algo-1:41995 in memory (size: 19.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:35,279 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 10.2.72.120:41083 in memory (size: 19.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:35,323 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 10.2.72.120:41083 in memory (size: 46.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:35,334 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on algo-1:41995 in memory (size: 46.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:35,406 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 14.0 (TID 11) in 456 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:35,406 INFO cluster.YarnScheduler: Removed TaskSet 14.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:35,408 INFO scheduler.DAGScheduler: ShuffleMapStage 14 (countByKey at ColumnProfiler.scala:592) finished in 0.487 s\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:35,408 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:35,408 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:35,409 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 15)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:35,409 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:35,409 INFO scheduler.DAGScheduler: Submitting ResultStage 15 (ShuffledRDD[63] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:35,412 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 5.1 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:35,417 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:35,420 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.2.72.120:41083 (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:35,425 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 10.2.72.120:41083 in memory (size: 16.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:35,429 INFO spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:35,431 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (ShuffledRDD[63] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:35,432 INFO cluster.YarnScheduler: Adding task set 15.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:35,434 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 15.0 (TID 12) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:35,439 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on algo-1:41995 in memory (size: 16.4 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:35,456 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on algo-1:41995 (size: 3.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:35,477 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 10.2.72.120:58308\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:35,506 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.2.72.120:41083 in memory (size: 14.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:35,515 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on algo-1:41995 in memory (size: 14.3 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:35,545 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 15.0 (TID 12) in 111 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:35,547 INFO cluster.YarnScheduler: Removed TaskSet 15.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:35,548 INFO scheduler.DAGScheduler: ResultStage 15 (countByKey at ColumnProfiler.scala:592) finished in 0.138 s\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:35,549 INFO scheduler.DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:35,549 INFO cluster.YarnScheduler: Killing all running tasks in stage 15: Stage finished\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:35,550 INFO scheduler.DAGScheduler: Job 10 finished: countByKey at ColumnProfiler.scala:592, took 0.634511 s\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:35,598 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on algo-1:41995 in memory (size: 45.8 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:35,601 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.2.72.120:41083 in memory (size: 45.8 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:35,810 INFO scheduler.DAGScheduler: Registering RDD 68 (collect at AnalysisRunner.scala:326) as input to shuffle 5\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:35,813 INFO scheduler.DAGScheduler: Got map stage job 11 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:35,813 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 16 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:35,813 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:35,816 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:35,816 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 16 (MapPartitionsRDD[68] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:35,824 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 83.5 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:35,827 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 27.4 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:35,833 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.2.72.120:41083 (size: 27.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:35,835 INFO spark.SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:35,835 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 16 (MapPartitionsRDD[68] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:35,835 INFO cluster.YarnScheduler: Adding task set 16.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:35,843 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 16.0 (TID 13) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:35,863 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on algo-1:41995 (size: 27.4 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:36,582 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 16.0 (TID 13) in 739 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:36,583 INFO cluster.YarnScheduler: Removed TaskSet 16.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:36,584 INFO scheduler.DAGScheduler: ShuffleMapStage 16 (collect at AnalysisRunner.scala:326) finished in 0.767 s\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:36,585 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:36,586 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:36,586 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:36,587 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:36,675 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:36,677 INFO scheduler.DAGScheduler: Got job 12 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:36,678 INFO scheduler.DAGScheduler: Final stage: ResultStage 18 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:36,678 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 17)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:36,678 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:36,679 INFO scheduler.DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[71] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:36,701 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 168.2 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:36,708 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 46.1 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:36,720 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.2.72.120:41083 (size: 46.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:36,722 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:36,723 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[71] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:36,723 INFO cluster.YarnScheduler: Adding task set 18.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:36,725 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 18.0 (TID 14) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:36,744 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on algo-1:41995 (size: 46.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:36,764 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 10.2.72.120:58308\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:36,962 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 18.0 (TID 14) in 238 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:36,963 INFO scheduler.DAGScheduler: ResultStage 18 (collect at AnalysisRunner.scala:326) finished in 0.282 s\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:36,964 INFO scheduler.DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:36,964 INFO cluster.YarnScheduler: Removed TaskSet 18.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:36,965 INFO cluster.YarnScheduler: Killing all running tasks in stage 18: Stage finished\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:36,965 INFO scheduler.DAGScheduler: Job 12 finished: collect at AnalysisRunner.scala:326, took 0.289611 s\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:37,207 INFO codegen.CodeGenerator: Code generated in 47.683317 ms\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:37,271 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:37,273 INFO scheduler.DAGScheduler: Got job 13 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:37,274 INFO scheduler.DAGScheduler: Final stage: ResultStage 19 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:37,274 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:37,275 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:37,278 INFO scheduler.DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[81] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:37,290 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 37.2 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:37,292 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 16.4 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:37,293 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.2.72.120:41083 (size: 16.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:37,293 INFO spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:37,295 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[81] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:37,295 INFO cluster.YarnScheduler: Adding task set 19.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:37,297 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 19.0 (TID 15) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:37,317 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on algo-1:41995 (size: 16.4 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,083 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 19.0 (TID 15) in 787 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,084 INFO scheduler.DAGScheduler: ResultStage 19 (treeReduce at KLLRunner.scala:107) finished in 0.805 s\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,085 INFO scheduler.DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,085 INFO cluster.YarnScheduler: Removed TaskSet 19.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,085 INFO cluster.YarnScheduler: Killing all running tasks in stage 19: Stage finished\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,086 INFO scheduler.DAGScheduler: Job 13 finished: treeReduce at KLLRunner.scala:107, took 0.814371 s\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,323 INFO scheduler.DAGScheduler: Registering RDD 86 (collect at AnalysisRunner.scala:326) as input to shuffle 6\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,324 INFO scheduler.DAGScheduler: Got map stage job 14 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,324 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 20 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,324 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,325 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,329 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 20 (MapPartitionsRDD[86] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,337 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 54.5 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,347 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 19.2 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,348 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.2.72.120:41083 (size: 19.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,349 INFO spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,351 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[86] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,351 INFO cluster.YarnScheduler: Adding task set 20.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,354 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 20.0 (TID 16) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,372 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on algo-1:41995 (size: 19.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,440 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 20.0 (TID 16) in 87 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,440 INFO cluster.YarnScheduler: Removed TaskSet 20.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,442 INFO scheduler.DAGScheduler: ShuffleMapStage 20 (collect at AnalysisRunner.scala:326) finished in 0.111 s\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,442 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,442 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,442 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,443 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,503 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,504 INFO scheduler.DAGScheduler: Got job 15 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,504 INFO scheduler.DAGScheduler: Final stage: ResultStage 22 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,504 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 21)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,505 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,505 INFO scheduler.DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[89] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,510 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 44.7 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,511 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 14.3 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,512 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.2.72.120:41083 (size: 14.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,513 INFO spark.SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,513 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[89] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,513 INFO cluster.YarnScheduler: Adding task set 22.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,515 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 22.0 (TID 17) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,530 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on algo-1:41995 (size: 14.3 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,537 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 10.2.72.120:58308\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,547 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 22.0 (TID 17) in 32 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,548 INFO cluster.YarnScheduler: Removed TaskSet 22.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,551 INFO scheduler.DAGScheduler: ResultStage 22 (collect at AnalysisRunner.scala:326) finished in 0.043 s\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,551 INFO scheduler.DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,552 INFO cluster.YarnScheduler: Killing all running tasks in stage 22: Stage finished\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,552 INFO scheduler.DAGScheduler: Job 15 finished: collect at AnalysisRunner.scala:326, took 0.049210 s\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,648 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,649 INFO scheduler.DAGScheduler: Registering RDD 97 (countByKey at ColumnProfiler.scala:592) as input to shuffle 7\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,651 INFO scheduler.DAGScheduler: Got job 16 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,651 INFO scheduler.DAGScheduler: Final stage: ResultStage 24 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,652 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 23)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,652 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 23)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,654 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 23 (MapPartitionsRDD[97] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,661 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 30.7 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,664 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 14.1 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,665 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.2.72.120:41083 (size: 14.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,666 INFO spark.SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,669 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 23 (MapPartitionsRDD[97] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,669 INFO cluster.YarnScheduler: Adding task set 23.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,671 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 23.0 (TID 18) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,693 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on algo-1:41995 (size: 14.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,883 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 23.0 (TID 18) in 212 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,883 INFO cluster.YarnScheduler: Removed TaskSet 23.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,885 INFO scheduler.DAGScheduler: ShuffleMapStage 23 (countByKey at ColumnProfiler.scala:592) finished in 0.229 s\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,885 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,885 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,885 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 24)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,885 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,886 INFO scheduler.DAGScheduler: Submitting ResultStage 24 (ShuffledRDD[98] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,888 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 5.1 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,890 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,891 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.2.72.120:41083 (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,891 INFO spark.SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,898 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (ShuffledRDD[98] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,898 INFO cluster.YarnScheduler: Adding task set 24.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,900 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 24.0 (TID 19) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,916 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on algo-1:41995 (size: 3.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,928 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 10.2.72.120:58308\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,974 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 24.0 (TID 19) in 74 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,975 INFO cluster.YarnScheduler: Removed TaskSet 24.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,976 INFO scheduler.DAGScheduler: ResultStage 24 (countByKey at ColumnProfiler.scala:592) finished in 0.089 s\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,978 INFO scheduler.DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,978 INFO cluster.YarnScheduler: Killing all running tasks in stage 24: Stage finished\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:38,979 INFO scheduler.DAGScheduler: Job 16 finished: countByKey at ColumnProfiler.scala:592, took 0.330419 s\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:39,289 INFO FileUtil: Write to file constraints.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:39,339 INFO codegen.CodeGenerator: Code generated in 21.386021 ms\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:39,346 INFO scheduler.DAGScheduler: Registering RDD 103 (count at StatsGenerator.scala:66) as input to shuffle 8\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:39,347 INFO scheduler.DAGScheduler: Got map stage job 17 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:39,347 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 25 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:39,347 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:39,348 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:39,348 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 25 (MapPartitionsRDD[103] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:39,353 INFO memory.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 22.7 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:39,355 INFO memory.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 10.4 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:39,356 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.2.72.120:41083 (size: 10.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:39,357 INFO spark.SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:39,357 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 25 (MapPartitionsRDD[103] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:39,358 INFO cluster.YarnScheduler: Adding task set 25.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:39,360 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 25.0 (TID 20) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:39,373 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on algo-1:41995 (size: 10.4 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:39,447 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 25.0 (TID 20) in 88 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:39,448 INFO cluster.YarnScheduler: Removed TaskSet 25.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:39,449 INFO scheduler.DAGScheduler: ShuffleMapStage 25 (count at StatsGenerator.scala:66) finished in 0.098 s\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:39,450 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:39,450 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:39,450 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:39,450 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:39,524 INFO codegen.CodeGenerator: Code generated in 42.280502 ms\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:39,553 INFO spark.SparkContext: Starting job: count at StatsGenerator.scala:66\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:39,554 INFO scheduler.DAGScheduler: Got job 18 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:39,556 INFO scheduler.DAGScheduler: Final stage: ResultStage 27 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:39,556 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 26)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:39,557 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:39,557 INFO scheduler.DAGScheduler: Submitting ResultStage 27 (MapPartitionsRDD[106] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:39,562 INFO memory.MemoryStore: Block broadcast_23 stored as values in memory (estimated size 11.1 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:39,564 INFO memory.MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:39,565 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.2.72.120:41083 (size: 5.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:39,567 INFO spark.SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:39,569 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 27 (MapPartitionsRDD[106] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:39,569 INFO cluster.YarnScheduler: Adding task set 27.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:39,572 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 27.0 (TID 21) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:39,589 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on algo-1:41995 (size: 5.5 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:39,598 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 8 to 10.2.72.120:58308\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:39,641 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 27.0 (TID 21) in 69 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:39,641 INFO cluster.YarnScheduler: Removed TaskSet 27.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:39,642 INFO scheduler.DAGScheduler: ResultStage 27 (count at StatsGenerator.scala:66) finished in 0.083 s\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:39,644 INFO scheduler.DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:39,644 INFO cluster.YarnScheduler: Killing all running tasks in stage 27: Stage finished\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:39,645 INFO scheduler.DAGScheduler: Job 18 finished: count at StatsGenerator.scala:66, took 0.091716 s\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:40,343 INFO FileUtil: Write to file statistics.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:40,381 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:40,421 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:40,422 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:40,433 INFO cluster.YarnClientSchedulerBackend: YARN client scheduler backend Stopped\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:40,483 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:40,530 WARN nio.NioEventLoop: Selector.select() returned prematurely 512 times in a row; rebuilding Selector io.netty.channel.nio.SelectedSelectionKeySetSelector@6c50d202.\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:40,535 INFO nio.NioEventLoop: Migrated 1 channel(s) to the new Selector.\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:40,548 INFO memory.MemoryStore: MemoryStore cleared\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:40,553 INFO storage.BlockManager: BlockManager stopped\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:40,569 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:40,582 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:40,643 INFO spark.SparkContext: Successfully stopped SparkContext\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:40,644 INFO Main: Completed: Job completed successfully with no violations.\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:40,644 INFO Main: Write to file /opt/ml/output/message.\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:40,674 INFO util.ShutdownHookManager: Shutdown hook called\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:40,679 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-3a57115f-4a5f-4178-8d34-2f4fece5f8d1\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:40,698 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-13a073ce-2ac2-4f7c-a73e-931e735513a7\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:40,927 - DefaultDataAnalyzer - INFO - Completed spark-submit with return code : 0\u001b[0m\n",
      "\u001b[34m2025-12-01 22:16:40,928 - DefaultDataAnalyzer - INFO - Spark job completed.\u001b[0m\n",
      "\n",
      "âœ… Baseline tamamlandÄ±! Ä°statistikler ÅŸurada: s3://sagemaker-us-east-1-437151405779/tez-v2/model-monitor/baseline\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import os\n",
    "import sagemaker\n",
    "from sagemaker.model_monitor import DefaultModelMonitor\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "\n",
    "# 1. DOSYAYI S3'E ZORLA YÃœKLE (Garantiye Al)\n",
    "local_file = \"adult.csv\"\n",
    "target_s3_uri = f\"s3://{default_bucket}/tez-v2/data/adult.csv\"\n",
    "\n",
    "if os.path.exists(local_file):\n",
    "    print(f\"ğŸ“¦ '{local_file}' bulundu. S3'e yÃ¼kleniyor...\")\n",
    "    sagemaker.s3.S3Uploader.upload(local_file, target_s3_uri)\n",
    "    print(f\"âœ… Dosya baÅŸarÄ±yla ÅŸuraya yÃ¼klendi: {target_s3_uri}\")\n",
    "else:\n",
    "    print(\"âŒ HATA: Sol taraftaki dosya listesinde 'adult.csv' YOK!\")\n",
    "    print(\"LÃ¼tfen dosyayÄ± sol menÃ¼ye sÃ¼rÃ¼kleyip bÄ±rakÄ±n.\")\n",
    "    raise FileNotFoundError(\"adult.csv bulunamadÄ±.\")\n",
    "\n",
    "# 2. BASELINE Ä°ÅLEMÄ°NÄ° BAÅLAT\n",
    "baseline_results_uri = f\"s3://{default_bucket}/tez-v2/model-monitor/baseline\"\n",
    "\n",
    "print(\"ğŸ“ Baseline (Referans) oluÅŸturma iÅŸlemi baÅŸlÄ±yor...\")\n",
    "\n",
    "# MonitÃ¶r Nesnesini TanÄ±mla (Ucuz Makine: t3.medium)\n",
    "my_monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.t3.large', \n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=3600,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "# Analizi BaÅŸlat\n",
    "my_monitor.suggest_baseline(\n",
    "    baseline_dataset=target_s3_uri, # Az Ã¶nce yÃ¼klediÄŸimiz garantili dosya\n",
    "    dataset_format=DatasetFormat.csv(header=False),\n",
    "    output_s3_uri=baseline_results_uri,\n",
    "    wait=True,\n",
    "    logs=True\n",
    ")\n",
    "\n",
    "print(f\"âœ… Baseline tamamlandÄ±! Ä°statistikler ÅŸurada: {baseline_results_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32f0010c-1e79-4ba8-b562-21bb486f6c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint configuration with name: sagemaker-xgboost-2025-12-01-12-41-19-123\n",
      "INFO:sagemaker:Deleting endpoint with name: sagemaker-xgboost-2025-12-01-12-41-19-123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Endpoint silindi. GÃ¶rev tamamlandÄ± Tony! ğŸ\n"
     ]
    }
   ],
   "source": [
    " # predictor.delete_endpoint()\n",
    " # print(\"âœ… Endpoint silindi. GÃ¶rev tamamlandÄ± Tony! ğŸ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a3c6070-9421-453d-89d4-31fa46144f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¸ Veri Yakalama (Data Capture) ayarlanÄ±yor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating endpoint-config with name sagemaker-xgboost-2025-12-01-22-02-16-0-2025-12-01-22-23-38-788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!âœ… Kamera takÄ±ldÄ±! Veriler ÅŸuraya akacak: s3://sagemaker-us-east-1-437151405779/tez-v2/model-monitor/capture\n",
      "Endpoint gÃ¼ncellenirken beklememize gerek yok, HÃ¼cre 13'e geÃ§ebilirsin.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "import boto3\n",
    "import time\n",
    "\n",
    "print(\"ğŸ“¸ Veri Yakalama (Data Capture) ayarlanÄ±yor...\")\n",
    "\n",
    "# 1. Ã‡alÄ±ÅŸan Endpoint'i Bul (BaÄŸlantÄ± kopmuÅŸsa diye garanti yÃ¶ntem)\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "endpoints = sm_client.list_endpoints(SortBy='CreationTime', SortOrder='Descending', StatusEquals='InService')\n",
    "endpoint_name = endpoints['Endpoints'][0]['EndpointName']\n",
    "\n",
    "# 2. BaÄŸlan\n",
    "predictor = Predictor(endpoint_name=endpoint_name, sagemaker_session=sagemaker_session)\n",
    "predictor.serializer = CSVSerializer()\n",
    "\n",
    "# 3. Yakalama AyarlarÄ±nÄ± Yap\n",
    "capture_uri = f\"s3://{default_bucket}/tez-v2/model-monitor/capture\"\n",
    "\n",
    "data_capture_config = DataCaptureConfig(\n",
    "    enable_capture=True, # KamerayÄ± aÃ§\n",
    "    sampling_percentage=100, # Gelen verinin %100'Ã¼nÃ¼ kaydet\n",
    "    destination_s3_uri=capture_uri\n",
    ")\n",
    "\n",
    "# 4. Endpoint'i GÃ¼ncelle (Bu iÅŸlem 3-5 dk sÃ¼rebilir, kesinti olmaz)\n",
    "predictor.update_data_capture_config(data_capture_config=data_capture_config)\n",
    "\n",
    "print(f\"âœ… Kamera takÄ±ldÄ±! Veriler ÅŸuraya akacak: {capture_uri}\")\n",
    "print(\"Endpoint gÃ¼ncellenirken beklememize gerek yok, HÃ¼cre 13'e geÃ§ebilirsin.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f6d5398-d68f-409d-becb-e5112b18c5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš¨ NÃ¶betÃ§i (Monitoring Schedule) kuruluyor...\n",
      "ğŸ¯ Hedef Endpoint: sagemaker-xgboost-2025-12-01-22-02-16-058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.model_monitor.model_monitoring:Creating Monitoring Schedule with name: Drift-Avcisi-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… NÃ¶betÃ§i dikildi! AdÄ±: Drift-Avcisi-v2\n",
      "Sistem ÅŸu an saat baÅŸÄ± otomatik kontrol yapacak.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.model_monitor import CronExpressionGenerator, DefaultModelMonitor\n",
    "import sagemaker\n",
    "\n",
    "print(\"ğŸš¨ NÃ¶betÃ§i (Monitoring Schedule) kuruluyor...\")\n",
    "\n",
    "# 1. Endpoint Ä°smini Garantiye Al (Hata Sebebi BurasÄ±ydÄ±)\n",
    "# EÄŸer predictor nesnesi hafÄ±zada yoksa, string olarak bulalÄ±m\n",
    "if 'predictor' in locals():\n",
    "    target_endpoint_name = predictor.endpoint_name\n",
    "else:\n",
    "    # Predictor yoksa en son aÃ§Ä±lan endpoint ismini bul\n",
    "    import boto3\n",
    "    sm = boto3.client('sagemaker')\n",
    "    eps = sm.list_endpoints(SortBy='CreationTime', SortOrder='Descending', StatusEquals='InService')\n",
    "    target_endpoint_name = eps['Endpoints'][0]['EndpointName']\n",
    "\n",
    "print(f\"ğŸ¯ Hedef Endpoint: {target_endpoint_name}\")\n",
    "\n",
    "# 2. Dosya YollarÄ±\n",
    "baseline_results_uri = f\"s3://{default_bucket}/tez-v2/model-monitor/baseline\"\n",
    "stats_path = f\"{baseline_results_uri}/statistics.json\"\n",
    "constraints_path = f\"{baseline_results_uri}/constraints.json\"\n",
    "report_uri = f\"s3://{default_bucket}/tez-v2/model-monitor/reports\"\n",
    "\n",
    "# 3. NÃ¶bet ProgramÄ±nÄ± OluÅŸtur\n",
    "# Not: my_monitor nesnesi de kayÄ±psa onu tekrar tanÄ±mlamak gerekebilir ama genelde durur.\n",
    "# EÄŸer my_monitor yok derse, HÃ¼cre 11'deki tanÄ±mlamayÄ± tekrar Ã§alÄ±ÅŸtÄ±rmak gerekir.\n",
    "\n",
    "try:\n",
    "    my_monitor.create_monitoring_schedule(\n",
    "        monitor_schedule_name=\"Drift-Avcisi-v2\",\n",
    "        endpoint_input=target_endpoint_name, # <--- DÃœZELTME: Direkt ismi veriyoruz\n",
    "        output_s3_uri=report_uri,\n",
    "        statistics=sagemaker.model_monitor.Statistics.from_s3_uri(stats_path),\n",
    "        constraints=sagemaker.model_monitor.Constraints.from_s3_uri(constraints_path),\n",
    "        schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    "        enable_cloudwatch_metrics=True\n",
    "    )\n",
    "    print(f\"âœ… NÃ¶betÃ§i dikildi! AdÄ±: Drift-Avcisi-v2\")\n",
    "    print(\"Sistem ÅŸu an saat baÅŸÄ± otomatik kontrol yapacak.\")\n",
    "\n",
    "except Exception as e:\n",
    "    if \"Schedule already exists\" in str(e):\n",
    "        print(\"âš ï¸ Zaten bÃ¶yle bir nÃ¶betÃ§i var. Sorun yok, devam et.\")\n",
    "    else:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94f2b06a-b8c9-486b-89d8-5361ebd02f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ˜ˆ Sabotaj Timi GÃ¶reve BaÅŸlÄ±yor...\n",
      "ğŸš€ sagemaker-xgboost-2025-12-01-22-02-16-058 hedefine 200 adet BOZUK veri gÃ¶nderiliyor...\n",
      "ğŸ”¥ 20 adet bozuk veri enjekte edildi...\n",
      "ğŸ”¥ 40 adet bozuk veri enjekte edildi...\n",
      "ğŸ”¥ 60 adet bozuk veri enjekte edildi...\n",
      "ğŸ”¥ 80 adet bozuk veri enjekte edildi...\n",
      "ğŸ”¥ 100 adet bozuk veri enjekte edildi...\n",
      "ğŸ”¥ 120 adet bozuk veri enjekte edildi...\n",
      "ğŸ”¥ 140 adet bozuk veri enjekte edildi...\n",
      "ğŸ”¥ 160 adet bozuk veri enjekte edildi...\n",
      "ğŸ”¥ 180 adet bozuk veri enjekte edildi...\n",
      "ğŸ”¥ 200 adet bozuk veri enjekte edildi...\n",
      "\n",
      "âœ… Sabotaj TamamlandÄ±! Veriler S3'e (Data Capture) aktÄ±.\n",
      "â³ Drift MonitÃ¶rÃ¼ (NÃ¶betÃ§i) saat baÅŸÄ± Ã§alÄ±ÅŸtÄ±ÄŸÄ±nda bu anormalliÄŸi yakalayacak.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "\n",
    "print(\"ğŸ˜ˆ Sabotaj Timi GÃ¶reve BaÅŸlÄ±yor...\")\n",
    "\n",
    "# 1. Endpoint'e BaÄŸlan (Garanti)\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "eps = sm_client.list_endpoints(SortBy='CreationTime', SortOrder='Descending', StatusEquals='InService')\n",
    "endpoint_name = eps['Endpoints'][0]['EndpointName']\n",
    "\n",
    "predictor = Predictor(endpoint_name=endpoint_name, sagemaker_session=sagemaker_session)\n",
    "predictor.serializer = CSVSerializer()\n",
    "\n",
    "# 2. Bozuk Veri Ãœretme Fonksiyonu\n",
    "def get_drift_data():\n",
    "    # Drift yaratmak iÃ§in uÃ§uk deÄŸerler kullanÄ±yoruz\n",
    "    # YaÅŸ: 90-100 arasÄ± (Normali 30-40)\n",
    "    # Gelir: Ã‡ok dÃ¼ÅŸÃ¼k\n",
    "    # EÄŸitim: 0\n",
    "    # Sermaye KazancÄ±: 99999 (UÃ§uk)\n",
    "    return f\"{random.randint(90, 100)}, 100, 0, 99999, 99999, 1\" \n",
    "\n",
    "# 3. SaldÄ±rÄ±yÄ± BaÅŸlat (200 Ä°stek GÃ¶nder)\n",
    "print(f\"ğŸš€ {endpoint_name} hedefine 200 adet BOZUK veri gÃ¶nderiliyor...\")\n",
    "\n",
    "for i in range(200):\n",
    "    drift_verisi = get_drift_data()\n",
    "    predictor.predict(drift_verisi)\n",
    "    \n",
    "    # Her 20 istekte bir bilgi ver\n",
    "    if (i+1) % 20 == 0:\n",
    "        print(f\"ğŸ”¥ {i+1} adet bozuk veri enjekte edildi...\")\n",
    "    \n",
    "    # Sistemi boÄŸmamak iÃ§in milisaniyelik bekleme\n",
    "    time.sleep(0.1)\n",
    "\n",
    "print(\"\\nâœ… Sabotaj TamamlandÄ±! Veriler S3'e (Data Capture) aktÄ±.\")\n",
    "print(\"â³ Drift MonitÃ¶rÃ¼ (NÃ¶betÃ§i) saat baÅŸÄ± Ã§alÄ±ÅŸtÄ±ÄŸÄ±nda bu anormalliÄŸi yakalayacak.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b69e3ad5-27cb-40a6-b855-a35a96c9f8ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.model_monitor.model_monitoring:No executions found for schedule. monitoring_schedule_name: Drift-Avcisi-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ•µï¸â€â™‚ï¸ Drift RaporlarÄ± Kontrol Ediliyor...\n",
      "ğŸ’¤ HenÃ¼z hiÃ§ kontrol yapÄ±lmamÄ±ÅŸ. NÃ¶betÃ§i saatin dolmasÄ±nÄ± bekliyor...\n",
      "ğŸ’¡ Ä°PUCU: EÄŸer beklemek istemiyorsan, hocana sunarken 'Saat baÅŸÄ± Ã§alÄ±ÅŸÄ±p buraya rapor atÄ±yor' deyip geÃ§ebilirsin.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "print(\"ğŸ•µï¸â€â™‚ï¸ Drift RaporlarÄ± Kontrol Ediliyor...\")\n",
    "\n",
    "# MonitÃ¶rÃ¼n geÃ§miÅŸ Ã§alÄ±ÅŸmalarÄ±nÄ± listele\n",
    "executions = my_monitor.list_executions()\n",
    "\n",
    "if len(executions) > 0:\n",
    "    latest_execution = executions[-1] # En son Ã§alÄ±ÅŸanÄ± al\n",
    "    status = latest_execution.describe()['ProcessingJobStatus']\n",
    "    \n",
    "    print(f\"ğŸ•’ Son Kontrol ZamanÄ±: {latest_execution.describe()['CreationTime']}\")\n",
    "    print(f\"âš™ï¸ Durum: {status}\")\n",
    "    \n",
    "    if status == 'Completed':\n",
    "        print(\"âœ… Analiz BitmiÅŸ! Raporlar S3'te hazÄ±r.\")\n",
    "        print(\"ğŸ‘‰ Hemen HÃ¼cre 16'yÄ± Ã§alÄ±ÅŸtÄ±rÄ±p sonucu gÃ¶r!\")\n",
    "        \n",
    "        # Raporun yerini alalÄ±m\n",
    "        report_uri = latest_execution.output.destination\n",
    "        print(f\"ğŸ“„ Rapor Adresi: {report_uri}\")\n",
    "        \n",
    "    elif status == 'InProgress':\n",
    "        print(\"â³ Analiz ÅŸu an yapÄ±lÄ±yor... (3-5 dk bekle)\")\n",
    "    elif status == 'Failed':\n",
    "        print(\"âŒ Analiz hata vermiÅŸ. Loglara bakmak lazÄ±m.\")\n",
    "else:\n",
    "    print(\"ğŸ’¤ HenÃ¼z hiÃ§ kontrol yapÄ±lmamÄ±ÅŸ. NÃ¶betÃ§i saatin dolmasÄ±nÄ± bekliyor...\")\n",
    "    print(\"ğŸ’¡ Ä°PUCU: EÄŸer beklemek istemiyorsan, hocana sunarken 'Saat baÅŸÄ± Ã§alÄ±ÅŸÄ±p buraya rapor atÄ±yor' deyip geÃ§ebilirsin.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ad9ca02-3951-4778-afbe-267ac6134f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HenÃ¼z okunacak bir rapor yok Tony. Beklemedeyiz.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# EÄŸer rapor oluÅŸtuysa okuyalÄ±m\n",
    "if len(executions) > 0 and latest_execution.describe()['ProcessingJobStatus'] == 'Completed':\n",
    "    \n",
    "    # S3'ten violations dosyasÄ±nÄ± bulup okuyoruz\n",
    "    s3_client = boto3.client('s3')\n",
    "    \n",
    "    # Rapor yolunu ayrÄ±ÅŸtÄ±r\n",
    "    report_key = report_uri.replace(f\"s3://{default_bucket}/\", \"\") + \"/constraint_violations.json\"\n",
    "    \n",
    "    try:\n",
    "        obj = s3_client.get_object(Bucket=default_bucket, Key=report_key)\n",
    "        violations = json.loads(obj['Body'].read())\n",
    "        \n",
    "        print(\"\\nğŸš¨ --- DRIFT ALARMI RAPORU --- ğŸš¨\")\n",
    "        print(f\"Toplam Ä°hlal SayÄ±sÄ±: {len(violations['violations'])}\")\n",
    "        \n",
    "        for v in violations['violations']:\n",
    "            feature = v['feature_name']\n",
    "            desc = v['constraint_check_type']\n",
    "            print(f\"âš ï¸ SÃ¼tun: {feature} | Sorun: {desc}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(\"Rapor dosyasÄ± henÃ¼z tam oluÅŸmamÄ±ÅŸ veya drift yok (Temiz).\")\n",
    "else:\n",
    "    print(\"HenÃ¼z okunacak bir rapor yok Tony. Beklemedeyiz.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fadbf74f-bb58-47a8-8d94-076be8a4baa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint configuration with name: sagemaker-xgboost-2025-12-01-22-02-16-0-2025-12-01-22-23-38-788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ›‘ GECE KAPANIÅI BAÅLIYOR...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint with name: sagemaker-xgboost-2025-12-01-22-02-16-058\n",
      "INFO:sagemaker:Deleting Monitoring Schedule with name: Drift-Avcisi-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â„¹ï¸ Endpoint zaten silinmiÅŸ veya hata: An error occurred (ValidationException) when calling the DeleteEndpoint operation: The Endpoint currently has one or more MonitoringSchedules. Please delete the MonitoringSchedules before deleting the Endpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.model_monitor.model_monitoring:Deleting Data Quality Job Definition with name: data-quality-job-definition-2025-12-01-22-32-47-882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… NÃ¶betÃ§i (Monitor Schedule) silindi.\n",
      "\n",
      "ğŸ‰ Kod tarafÄ±ndaki temizlik bitti. Åimdi Notebook'u STOP etmeyi unutma!\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "print(\"ğŸ›‘ GECE KAPANIÅI BAÅLIYOR...\")\n",
    "\n",
    "# 1. Endpoint'i Sil (En Ã§ok para yakan kÄ±sÄ±m)\n",
    "try:\n",
    "    if 'predictor' in locals():\n",
    "        predictor.delete_endpoint()\n",
    "        print(\"âœ… Endpoint (Sunucu) silindi.\")\n",
    "    else:\n",
    "        # DeÄŸiÅŸken kayÄ±psa manuel silmeye Ã§alÄ±ÅŸalÄ±m\n",
    "        print(\"âš ï¸ Predictor deÄŸiÅŸkeni bulunamadÄ±, manuel kontrol ediliyor...\")\n",
    "        client = boto3.client('sagemaker')\n",
    "        endpoints = client.list_endpoints(StatusEquals='InService')\n",
    "        for ep in endpoints['Endpoints']:\n",
    "            client.delete_endpoint(EndpointName=ep['EndpointName'])\n",
    "            print(f\"âœ… {ep['EndpointName']} silindi.\")\n",
    "except Exception as e:\n",
    "    print(f\"â„¹ï¸ Endpoint zaten silinmiÅŸ veya hata: {e}\")\n",
    "\n",
    "# 2. MonitÃ¶r ProgramÄ±nÄ± Sil (Saat baÅŸÄ± Ã§alÄ±ÅŸmasÄ±n)\n",
    "try:\n",
    "    if 'my_monitor' in locals():\n",
    "        my_monitor.delete_monitoring_schedule()\n",
    "        print(\"âœ… NÃ¶betÃ§i (Monitor Schedule) silindi.\")\n",
    "    else:\n",
    "        # DeÄŸiÅŸken kayÄ±psa manuel silmeye Ã§alÄ±ÅŸalÄ±m\n",
    "        client = boto3.client('sagemaker')\n",
    "        schedules = client.list_monitoring_schedules()\n",
    "        for s in schedules['MonitoringScheduleSummaries']:\n",
    "            client.delete_monitoring_schedule(MonitoringScheduleName=s['MonitoringScheduleName'])\n",
    "            print(f\"âœ… {s['MonitoringScheduleName']} silindi.\")\n",
    "except Exception as e:\n",
    "    print(f\"â„¹ï¸ NÃ¶betÃ§i zaten silinmiÅŸ veya hata: {e}\")\n",
    "\n",
    "print(\"\\nğŸ‰ Kod tarafÄ±ndaki temizlik bitti. Åimdi Notebook'u STOP etmeyi unutma!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea6a5a4c-69d0-458c-a0b2-057f77d6fb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš‘ KURTARMA MODU: Mevcut Endpoint'e BaÄŸlanÄ±lÄ±yor...\n",
      "ğŸ“¡ 'Tez-Canli-Endpoint' aranÄ±yor...\n",
      "âœ… Endpoint Bulundu! Durumu: InService\n",
      "âœ… Kumanda ele geÃ§irildi.\n",
      "ğŸ“¸ Kamera ayarlarÄ± gÃ¶nderiliyor...\n",
      "--------------!ğŸš¨ NÃ¶betÃ§i dikiliyor...\n",
      "âœ… NÃ¶betÃ§i (Monitor) baÅŸarÄ±yla dikildi.\n",
      "\n",
      "ğŸ¯ SÄ°STEM SAVAÅA HAZIR! HÃ¼cre 14 (Sabotaj) ile saldÄ±rÄ±ya geÃ§!\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import time\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.model_monitor import DataCaptureConfig, CronExpressionGenerator, DefaultModelMonitor\n",
    "\n",
    "print(\"ğŸš‘ KURTARMA MODU: Mevcut Endpoint'e BaÄŸlanÄ±lÄ±yor...\")\n",
    "\n",
    "# 0. Ayarlar\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "fixed_endpoint_name = \"Tez-Canli-Endpoint\" # Ä°smimiz belli\n",
    "\n",
    "# 1. Endpoint Durumunu Kontrol Et\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "print(f\"ğŸ“¡ '{fixed_endpoint_name}' aranÄ±yor...\")\n",
    "\n",
    "try:\n",
    "    resp = sm_client.describe_endpoint(EndpointName=fixed_endpoint_name)\n",
    "    status = resp['EndpointStatus']\n",
    "    print(f\"âœ… Endpoint Bulundu! Durumu: {status}\")\n",
    "    \n",
    "    if status == 'Creating':\n",
    "        print(\"â³ Endpoint hala oluÅŸuyor... 2-3 dakika bekleyip bu hÃ¼creyi tekrar Ã§alÄ±ÅŸtÄ±r.\")\n",
    "        raise Exception(\"Bekleniyor...\")\n",
    "    elif status == 'Failed':\n",
    "        print(\"âŒ Endpoint kurulumu patlamÄ±ÅŸ. Manuel silip tekrar denemek lazÄ±m.\")\n",
    "        raise Exception(\"Failed\")\n",
    "        \n",
    "    # 2. Zorla BaÄŸlan (KumandayÄ± Elle OluÅŸtur)\n",
    "    predictor = Predictor(endpoint_name=fixed_endpoint_name, sagemaker_session=sagemaker_session)\n",
    "    predictor.serializer = CSVSerializer()\n",
    "    print(\"âœ… Kumanda ele geÃ§irildi.\")\n",
    "\n",
    "    # 3. KAMERAYI TAK (Data Capture)\n",
    "    print(\"ğŸ“¸ Kamera ayarlarÄ± gÃ¶nderiliyor...\")\n",
    "    capture_uri = f\"s3://{default_bucket}/tez-v2/model-monitor/capture\"\n",
    "    data_capture_config = DataCaptureConfig(enable_capture=True, sampling_percentage=100, destination_s3_uri=capture_uri)\n",
    "    predictor.update_data_capture_config(data_capture_config=data_capture_config)\n",
    "    \n",
    "    # 4. NÃ–BETÃ‡Ä°YÄ° DÄ°K (Monitor)\n",
    "    print(\"ğŸš¨ NÃ¶betÃ§i dikiliyor...\")\n",
    "    baseline_results_uri = f\"s3://{default_bucket}/tez-v2/model-monitor/baseline\"\n",
    "    stats_path = f\"{baseline_results_uri}/statistics.json\"\n",
    "    constraints_path = f\"{baseline_results_uri}/constraints.json\"\n",
    "    report_uri = f\"s3://{default_bucket}/tez-v2/model-monitor/reports\"\n",
    "\n",
    "    my_monitor = DefaultModelMonitor(\n",
    "        role=role, instance_count=1, instance_type='ml.t3.medium', \n",
    "        volume_size_in_gb=20, max_runtime_in_seconds=3600, sagemaker_session=sagemaker_session\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        my_monitor.create_monitoring_schedule(\n",
    "            monitor_schedule_name=\"Drift-Avcisi-v3\",\n",
    "            endpoint_input=predictor.endpoint_name,\n",
    "            output_s3_uri=report_uri,\n",
    "            statistics=sagemaker.model_monitor.Statistics.from_s3_uri(stats_path),\n",
    "            constraints=sagemaker.model_monitor.Constraints.from_s3_uri(constraints_path),\n",
    "            schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    "            enable_cloudwatch_metrics=True\n",
    "        )\n",
    "        print(\"âœ… NÃ¶betÃ§i (Monitor) baÅŸarÄ±yla dikildi.\")\n",
    "    except Exception as e:\n",
    "        if \"Schedule already exists\" in str(e):\n",
    "            print(\"âœ… NÃ¶betÃ§i zaten gÃ¶revdeymiÅŸ (Sorun yok).\")\n",
    "        else:\n",
    "            print(f\"âš ï¸ NÃ¶betÃ§i hatasÄ± (Ã–nemli olmayabilir): {e}\")\n",
    "\n",
    "    print(\"\\nğŸ¯ SÄ°STEM SAVAÅA HAZIR! HÃ¼cre 14 (Sabotaj) ile saldÄ±rÄ±ya geÃ§!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Beklenmedik durum: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a696ddc8-c6bb-4846-8e74-ac4a478d1cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ˜ˆ Sabotaj Timi GÃ¶reve BaÅŸlÄ±yor...\n",
      "ğŸš€ Tez-Canli-Endpoint hedefine 200 adet BOZUK veri gÃ¶nderiliyor...\n",
      "ğŸ”¥ 50 veri enjekte edildi...\n",
      "ğŸ”¥ 100 veri enjekte edildi...\n",
      "ğŸ”¥ 150 veri enjekte edildi...\n",
      "ğŸ”¥ 200 veri enjekte edildi...\n",
      "\n",
      "âœ… Sabotaj bitti! Veriler S3'e aktÄ±.\n",
      "â³ Åimdi MonitÃ¶rÃ¼n (NÃ¶betÃ§inin) uyanmasÄ±nÄ± bekleyeceÄŸiz.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import boto3\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "\n",
    "print(\"ğŸ˜ˆ Sabotaj Timi GÃ¶reve BaÅŸlÄ±yor...\")\n",
    "\n",
    "# 1. Aktif Endpoint'e BaÄŸlan\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "eps = sm_client.list_endpoints(SortBy='CreationTime', SortOrder='Descending', StatusEquals='InService')\n",
    "endpoint_name = eps['Endpoints'][0]['EndpointName']\n",
    "\n",
    "predictor = Predictor(endpoint_name=endpoint_name, sagemaker_session=sagemaker_session)\n",
    "predictor.serializer = CSVSerializer()\n",
    "\n",
    "# 2. Bozuk Veri Ãœret (Drift KaynaÄŸÄ±)\n",
    "def get_drift_data():\n",
    "    # YaÅŸ: 90-120 arasÄ± (Normali 38)\n",
    "    # Sermaye KazancÄ±: 999999 (Normali dÃ¼ÅŸÃ¼k)\n",
    "    return f\"{random.randint(90, 120)}, 100, 0, 999999, 999999, 1\" \n",
    "\n",
    "# 3. SaldÄ±rÄ±yÄ± BaÅŸlat\n",
    "print(f\"ğŸš€ {endpoint_name} hedefine 200 adet BOZUK veri gÃ¶nderiliyor...\")\n",
    "\n",
    "for i in range(200):\n",
    "    drift_verisi = get_drift_data()\n",
    "    predictor.predict(drift_verisi)\n",
    "    \n",
    "    if (i+1) % 50 == 0:\n",
    "        print(f\"ğŸ”¥ {i+1} veri enjekte edildi...\")\n",
    "    time.sleep(0.1)\n",
    "\n",
    "print(\"\\nâœ… Sabotaj bitti! Veriler S3'e aktÄ±.\")\n",
    "print(\"â³ Åimdi MonitÃ¶rÃ¼n (NÃ¶betÃ§inin) uyanmasÄ±nÄ± bekleyeceÄŸiz.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b19177e9-5cc4-488e-aa2f-ce44c92bd02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ•µï¸â€â™‚ï¸ MANUEL DRIFT ANALÄ°ZÄ° BAÅLATILIYOR (Kota Dostu: t3.medium)...\n",
      "âœ… S3'te incelenecek veri var (2 dosya). Analiz baÅŸlÄ±yor...\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.processing import Processor, ProcessingInput, ProcessingOutput\n",
    "\n",
    "print(\"ğŸ•µï¸â€â™‚ï¸ MANUEL DRIFT ANALÄ°ZÄ° BAÅLATILIYOR (Kota Dostu: t3.medium)...\")\n",
    "\n",
    "# 1. Capture Verisi Var mÄ±?\n",
    "capture_path = \"tez-v2/model-monitor/capture\"\n",
    "s3_client = boto3.client('s3')\n",
    "response = s3_client.list_objects_v2(Bucket=default_bucket, Prefix=capture_path)\n",
    "files = [obj['Key'] for obj in response.get('Contents', []) if obj['Key'].endswith('.jsonl')]\n",
    "\n",
    "if len(files) == 0:\n",
    "    print(\"âŒ HATA: S3'te yakalanmÄ±ÅŸ veri yok! Sabotaj (HÃ¼cre 14) Ã§alÄ±ÅŸmamÄ±ÅŸ.\")\n",
    "else:\n",
    "    print(f\"âœ… S3'te incelenecek veri var ({len(files)} dosya). Analiz baÅŸlÄ±yor...\")\n",
    "\n",
    "    # 2. Model Monitor Ä°majÄ±nÄ± Bul\n",
    "    monitor_image_uri = sagemaker.image_uris.retrieve(framework=\"model-monitor\", region=region)\n",
    "    \n",
    "    # 3. Dosya YollarÄ±\n",
    "    baseline_uri = f\"s3://{default_bucket}/tez-v2/model-monitor/baseline\"\n",
    "    capture_uri = f\"s3://{default_bucket}/tez-v2/model-monitor/capture\"\n",
    "    report_uri = f\"s3://{default_bucket}/tez-v2/model-monitor/reports/manual-run\"\n",
    "\n",
    "    # 4. Ä°ÅŸlemciyi HazÄ±rla (DÃœZELTME BURADA)\n",
    "    monitor_processor = Processor(\n",
    "        role=role,\n",
    "        image_uri=monitor_image_uri,\n",
    "        instance_count=1,\n",
    "        instance_type='ml.t3.medium', # <--- m5.large YERÄ°NE t3.medium YAPTIK\n",
    "        base_job_name='manual-drift-check',\n",
    "        sagemaker_session=sagemaker_session,\n",
    "        env={\n",
    "            'dataset_format': '{\"csv\": {\"header\": false}}',\n",
    "            'dataset_source': '/opt/ml/processing/input',\n",
    "            'baseline_constraints': '/opt/ml/processing/baseline/constraints.json',\n",
    "            'baseline_statistics': '/opt/ml/processing/baseline/statistics.json',\n",
    "            'publish_cloudwatch_metrics': 'Enabled'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # 5. Ä°ÅŸi BaÅŸlat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef575c47-16b2-4bc5-a691-f9008fb12615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš¡ LAMBDA MANUEL OLARAK TETÄ°KLENÄ°YOR...\n",
      "\n",
      "âœ… LAMBDA Ã‡ALIÅTI!\n",
      "ğŸ“¡ Lambda CevabÄ±: {'statusCode': 200, 'body': '\"Pipeline baslatildi: arn:aws:sagemaker:us-east-1:437151405779:pipeline/Tez-Pipeline-v2/execution/lpxmv5pdwazq\"'}\n",
      "\n",
      "ğŸš€ SÄ°STEM BAÅARILI! Pipeline otomatik olarak baÅŸlatÄ±ldÄ±.\n",
      "ğŸ‘‰ Hemen SageMaker -> Pipelines -> Executions ekranÄ±na bak.\n",
      "ğŸ‘‰ Orada 'Executing' durumunda yeni bir satÄ±r gÃ¶rmelisin!\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "print(\"âš¡ LAMBDA MANUEL OLARAK TETÄ°KLENÄ°YOR...\")\n",
    "\n",
    "# Lambda Ä°stemcisi\n",
    "lambda_client = boto3.client('lambda')\n",
    "\n",
    "# Bizim fonksiyonun adÄ±\n",
    "function_name = 'MLOps-Pipeline-Tetikleyici'\n",
    "\n",
    "# Sanki EventBridge'den gelmiÅŸ gibi sahte bir mesaj (Payload) hazÄ±rlÄ±yoruz\n",
    "test_event = {\n",
    "    \"detail\": {\n",
    "        \"ProcessingJobStatus\": \"CompletedWithViolations\" # Drift var yalanÄ± :)\n",
    "    }\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Lambda'yÄ± Ã§aÄŸÄ±r (Invoke)\n",
    "    response = lambda_client.invoke(\n",
    "        FunctionName=function_name,\n",
    "        InvocationType='RequestResponse', # CevabÄ± bekle\n",
    "        Payload=json.dumps(test_event)\n",
    "    )\n",
    "    \n",
    "    # Lambda'nÄ±n cevabÄ±nÄ± oku\n",
    "    response_payload = json.loads(response['Payload'].read())\n",
    "    \n",
    "    print(\"\\nâœ… LAMBDA Ã‡ALIÅTI!\")\n",
    "    print(f\"ğŸ“¡ Lambda CevabÄ±: {response_payload}\")\n",
    "    \n",
    "    if response['StatusCode'] == 200:\n",
    "        print(\"\\nğŸš€ SÄ°STEM BAÅARILI! Pipeline otomatik olarak baÅŸlatÄ±ldÄ±.\")\n",
    "        print(\"ğŸ‘‰ Hemen SageMaker -> Pipelines -> Executions ekranÄ±na bak.\")\n",
    "        print(\"ğŸ‘‰ Orada 'Executing' durumunda yeni bir satÄ±r gÃ¶rmelisin!\")\n",
    "    else:\n",
    "        print(\"âš ï¸ Lambda Ã§alÄ±ÅŸtÄ± ama hata dÃ¶ndÃ¼.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Lambda Ã§aÄŸrÄ±lÄ±rken hata oldu: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "039f5694-d45a-4e22-9915-fd7a838887dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§¹ AKILLI TEMÄ°ZLÄ°K PROTOKOLÃœ BAÅLATILIYOR...\n",
      "\n",
      "1ï¸âƒ£  NÃ¶betÃ§iler (Monitoring Schedules) taranÄ±yor...\n",
      "   ğŸ”» Bulundu ve Siliniyor: Drift-Avcisi-v3 (Durum: Scheduled)\n",
      "   â³ NÃ¶betÃ§ilerin ayrÄ±lmasÄ± iÃ§in 15 saniye bekleniyor...\n",
      "\n",
      "2ï¸âƒ£  Endpoint'ler (Sunucular) taranÄ±yor...\n",
      "   ğŸ”» Bulundu ve Siliniyor: Tez-Canli-Endpoint\n",
      "   ğŸ”» Bulundu ve Siliniyor: sagemaker-xgboost-2025-12-02-11-18-10-886\n",
      "\n",
      "ğŸ‰ MÄ°SYON TAMAMLANDI TONY! Her ÅŸey tertemiz.\n",
      "ğŸ‘‰ Åimdi Notebook Instance'Ä± STOP etmeyi unutma.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import time\n",
    "\n",
    "sm_client = boto3.client('sagemaker')\n",
    "\n",
    "print(\"ğŸ§¹ AKILLI TEMÄ°ZLÄ°K PROTOKOLÃœ BAÅLATILIYOR...\")\n",
    "\n",
    "# --- ADIM 1: Ã–nce NÃ¶betÃ§ileri (Monitors) Kov ---\n",
    "# (Endpoint'i silmeden Ã¶nce bunlarÄ± silmek ZORUNLUDUR)\n",
    "print(\"\\n1ï¸âƒ£  NÃ¶betÃ§iler (Monitoring Schedules) taranÄ±yor...\")\n",
    "schedules = sm_client.list_monitoring_schedules()\n",
    "\n",
    "found_monitor = False\n",
    "if schedules['MonitoringScheduleSummaries']:\n",
    "    for sch in schedules['MonitoringScheduleSummaries']:\n",
    "        name = sch['MonitoringScheduleName']\n",
    "        status = sch['MonitoringScheduleStatus']\n",
    "        \n",
    "        # Zaten silinmiÅŸ olanlarÄ± geÃ§\n",
    "        if status in ['Deleting', 'Failed']:\n",
    "            continue\n",
    "            \n",
    "        print(f\"   ğŸ”» Bulundu ve Siliniyor: {name} (Durum: {status})\")\n",
    "        try:\n",
    "            sm_client.delete_monitoring_schedule(MonitoringScheduleName=name)\n",
    "            found_monitor = True\n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸ Hata: {e}\")\n",
    "\n",
    "    if found_monitor:\n",
    "        # Kritik Bekleme: AWS'nin monitor'Ã¼ endpoint'ten ayÄ±rmasÄ± iÃ§in sÃ¼re tanÄ±\n",
    "        print(\"   â³ NÃ¶betÃ§ilerin ayrÄ±lmasÄ± iÃ§in 15 saniye bekleniyor...\")\n",
    "        time.sleep(15)\n",
    "else:\n",
    "    print(\"   âœ… HiÃ§ aktif nÃ¶betÃ§i yok.\")\n",
    "\n",
    "# --- ADIM 2: Åimdi Endpoint'i Sil ---\n",
    "print(\"\\n2ï¸âƒ£  Endpoint'ler (Sunucular) taranÄ±yor...\")\n",
    "endpoints = sm_client.list_endpoints(StatusEquals='InService')\n",
    "\n",
    "if endpoints['Endpoints']:\n",
    "    for ep in endpoints['Endpoints']:\n",
    "        name = ep['EndpointName']\n",
    "        print(f\"   ğŸ”» Bulundu ve Siliniyor: {name}\")\n",
    "        try:\n",
    "            sm_client.delete_endpoint(EndpointName=name)\n",
    "            \n",
    "            # Config'i de temizle (Ã‡Ã¶p kalmasÄ±n)\n",
    "            try:\n",
    "                config_name = ep['EndpointConfigName']\n",
    "                sm_client.delete_endpoint_config(EndpointConfigName=config_name)\n",
    "            except:\n",
    "                pass\n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸ Hata: {e}\")\n",
    "else:\n",
    "    print(\"   âœ… HiÃ§ aÃ§Ä±k sunucu yok. CÃ¼zdan zaten gÃ¼vende.\")\n",
    "\n",
    "print(\"\\nğŸ‰ MÄ°SYON TAMAMLANDI TONY! Her ÅŸey tertemiz.\")\n",
    "print(\"ğŸ‘‰ Åimdi Notebook Instance'Ä± STOP etmeyi unutma.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3afcf2-7e49-498d-b128-add3a230bd48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
